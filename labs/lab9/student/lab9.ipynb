{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9\n",
    "\n",
    "**Note: Gradescope autograder will not work with this notebook because of the difference in how command line commands run in your notebook vs. on Gradescope. You will rely soley on the built-in notebook for feedback.**\n",
    "\n",
    "**Because this, Lab 9 will not be graded; however, you are expected to know the material**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 1a: Downloading from URL\n",
    "\n",
    "So far, we used `wget` to download from URLs. There is another command, `curl`, that also downloads from URLs. In this assignment, we will use `curl` and other command line tools to scrape [annual air quality summary data from EPA](https://aqs.epa.gov/aqsweb/airdata/download_files.html#Annual).\n",
    "\n",
    "![aqi-chart](images/aqi-chart.png)\n",
    "\n",
    "* Visit and [inspect the source code](https://www.lifewire.com/view-web-source-code-4151702) of this EPA webpage: https://aqs.epa.gov/aqsweb/airdata/download_files.html\n",
    "* Use `curl` command to download the same URL and [save the output to a file using `>` redirect](http://swcarpentry.github.io/shell-novice/04-pipefilter/index.html). Name the output file `files.html`\n",
    "* Use `tail` to print 10 last lines of `files.html`\n",
    "\n",
    "_Use `!` or `%%bash` when running any shell command inside the notebook, and replace `# use [command] here` with your answer_\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "manual: false\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -s https://... > files.html # complete the URL of `download_files.html` and redirect to `files.html`\n",
    "! tail -n10 files.html # use tail to show last 10 lines of `files.html`\n",
    "! tail -n10 files.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 1b: Chaining commands\n",
    "\n",
    "Instead of saving the output to file, then printing the last ten lines, you can execute a [sequence of commands together using pipes](http://swcarpentry.github.io/shell-novice/04-pipefilter/index.html). Using pipes, write a one line shell command to replace Question 1a.\n",
    "\n",
    "You can see from the output that the download progress is printed to screen. Something like this:\n",
    "```\n",
    "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "                                 Dload  Upload   Total   Spent    Left  Speed\n",
    "100  307k  100  307k    0     0   319k      0 --:--:-- --:--:-- --:--:--  319k\n",
    "```\n",
    "We don't want this to be a part of the html code, so use `curl`'s silent mode to eliminate the progress information.\n",
    "\n",
    "Save the result to a python variable named `lastlines`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastlines = ! curl -s https://... | tail -n10 # combine 1a into one command\n",
    "print('\\n'.join(lastlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 2a: `grep`\n",
    "\n",
    "The `grep` command searches lines in a text file. Inspect what kinds of options are available using the help page: `grep --help`.\n",
    "\n",
    "Use `grep` to find lines with the substring `zip` and print first five lines using `head`. To avoid downloading the html page over and over again, use `files.html` as your input to `grep`.\n",
    "\n",
    "Save the resulting list of strings to a python variable named `ziplines`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "manual: true\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ziplines = ! grep ... files.html | head -n5 # look for lines containing string, `zip`\n",
    "print('\\n'.join(ziplines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 2b: Regular expression\n",
    "\n",
    "Note how the word `zip` doesn't just appear as a part of zip file names. We want our `grep` to match more specific pattern that looks like zip file names. The part of the html that becomes the download link is [`href`](https://www.w3schools.com/tags/tryit.asp?filename=tryhtml_a_href). Let's search for filenames specified by `href`. \n",
    "\n",
    "Take, for example, the following line:\n",
    "```\n",
    "<a href=\"aqs_monitors.zip\">Monitor Listing</a> (353,283 Rows, 6,307 KB)<BR><BR>\n",
    "```\n",
    "There are many different and better ways of doing this. We will use `grep` and regular expression patterns to extract the links. A good way to learn regular expression is to know the basics and experiment.\n",
    "\n",
    "* [Quick reference of regular expression pattern](https://cheatography.com/davechild/cheat-sheets/regular-expressions/) showing the basic building blocks\n",
    "* [Online utility for testing regular expression patterns](https://regexr.com)\n",
    "\n",
    "One quick way could be to find the pattern `href=\"[some file name]` to find all substrings containing links. Then filter the lines again that contain the word `zip`.\n",
    "\n",
    "Follow this sequence of commands using pipes:\n",
    "1. **Find lines with substring`zip` using `grep` and `files.html`.**  \n",
    "    First few lines would look similar to,\n",
    "```\n",
    "    the size of the (zipped) file, the number of data rows in the file,\n",
    "<a href=\"aqs_sites.zip\">Site Listing</a> (20,611 Rows, 982 KB)<BR>\n",
    "<a href=\"aqs_monitors.zip\">Monitor Listing</a> (353,283 Rows, 6,307 KB)<BR><BR>\n",
    "            <TD style=\"font-size:90%;text-align:center;\"><a href=\"annual_conc_by_monitor_2019.zip\">annual_conc_by_monitor_2019.zip</a><BR>51,707 Rows<BR>3,081 KB<BR>As of 2019-11-13</TD>\n",
    "```\n",
    "1. **Find lines with `grep` and a [regular expression](http://swcarpentry.github.io/shell-novice/07-find/index.html) pattern that matches URLs: `href=\"[some-url]\"`.**  \n",
    "    _The pattern `'href=\\\"[^\\\"]*'` would matches any `href=\"` and following string until another quote appears: e.g. `[^a]` in regular expression means not to match a. When using `grep` with regular expressions, it is easier to use `-E` option (for extended regular expression)._ After using `grep` option for extracting just the matching substrings (`-o`), you would get something similar to this:\n",
    "```\n",
    "href=\"aqs_sites.zip\n",
    "href=\"aqs_monitors.zip\n",
    "href=\"annual_conc_by_monitor_2019.zip\n",
    "href=\"annual_aqi_by_cbsa_2019.zip\n",
    "href=\"annual_aqi_by_county_2019.zip    \n",
    "```\n",
    "1. **Strip `href=\"` by using `sed`**  \n",
    "    We want just the URLs, so strip the unnecessary `href=` portion. Test using something like `echo \"hello there\" | sed 's/ll/rr/'`. For example, giving `'s/href=\\\"//g'` tells sed to replace `href=\"` with an empty string (`rr` is an empty string) globally (trailing `g`).  \n",
    "    After running `sed`, you would get something like this:\n",
    "```\n",
    "aqs_sites.zip\n",
    "aqs_monitors.zip\n",
    "annual_conc_by_monitor_2019.zip\n",
    "annual_aqi_by_cbsa_2019.zip\n",
    "annual_aqi_by_county_2019.zip    \n",
    "```\n",
    "    [_Refer to Unix Power Tools for more information (UCSB NetID required)_](https://proquest-safaribooksonline-com.proxy.library.ucsb.edu:9443/book/operating-systems-and-server-administration/unix/0596003307/34dot-the-sed-stream-editor/upt3_chp_34_sect_3_html)\n",
    "1. **Save the resulting list of strings (each string is a URL for a zip file) a python variable named `flist`**\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b\n",
    "manual: true\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment one # at a time to see intermediate results\n",
    "flist = ! grep ... files.html \\\n",
    "#    | grep -o -E ... \\\n",
    "#    | sed ...\n",
    "print('\\n'.join(flist[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## Question 3\n",
    "\n",
    "In the previous question, we created a variable `flist` that contains 1705 file names by scraping a webpage. Hypothetically, you can loop through the list variable and download any file you need for your analysis.\n",
    "\n",
    "In this problem, we will work with a small portion of the data: `https://aqs.epa.gov/aqsweb/airdata/annual_aqi_by_county_2019.zip`. Download this using `wget` and unzip it to find `annual_aqi_by_county_2019.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 3a: Inspecting the Header\n",
    "\n",
    "Use `head` to print the header line of `annual_aqi_by_county_2019.csv` and use `sed` to print each column into one line. You can replace commas (`,`) with a newline character (`\\n`). Save to a python list variable named `aqiheader`. Each header name will become a list element.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a\n",
    "manual: true\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqiheader = ! head -n ... annual_aqi_by_county_2019.csv | sed 's/.../.../g' # your command\n",
    "print('\\n'.join(aqiheader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 3b: Count Locations\n",
    "\n",
    "How many counties are there in each state? Each row is a county, so you can extract the `State` column and count how many rows there are for each state using `uniq`. Then, use `cat` to number the output lines. Save the result to a python variable `county_counts`. \n",
    "\n",
    "First few lines should look like this:\n",
    "```\n",
    "   1\t     16 \"Alabama\"\n",
    "   2\t      6 \"Alaska\"\n",
    "   3\t     13 \"Arizona\"\n",
    "   4\t     13 \"Arkansas\"\n",
    "   5\t     53 \"California\"\n",
    "```\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b\n",
    "manual: true\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment one line at a time to see what each line does\n",
    "county_counts = ! cut -d '...' -f ... annual_aqi_by_county_2019.csv \\\n",
    "#    | grep -v 'State' \\\n",
    "#    | uniq -c \\\n",
    "#    | cat -n\n",
    "print('\\n'.join(county_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 3c: View CSV in Table\n",
    "\n",
    "Take first 5 lines of `annual_aqi_by_county_2019.csv`, extract first seven columns, and use `column` command to view data in a table. Assign the result to a python variable named `aqi_table`. The result looks like this:\n",
    "```\n",
    "\"State\"    \"County\"   \"Year\"  \"Days with AQI\"  \"Good Days\"  \"Moderate Days\"  \"Unhealthy for Sensitive Groups Days\"\n",
    "\"Alabama\"  \"Baldwin\"  2019    166              140          26               0\n",
    "\"Alabama\"  \"Clay\"     2019    63               58           5                0\n",
    "\"Alabama\"  \"Colbert\"  2019    171              161          10               0\n",
    "\"Alabama\"  \"DeKalb\"   2019    208              188          20               0\n",
    "```\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3c\n",
    "manual: true\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(aqi_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(\"lab9.ipynb\", filtering=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
